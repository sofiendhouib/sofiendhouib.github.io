I was a research scientist and a member of the [Decision Making team](https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/decision-making/) of the Tuebingen AI center from Mai 1, 2021 to April 30, 2024. Before that, I was a PhD at the [CREATIS laboratory](https://www.creatis.insa-lyon.fr/site7/en) under the supervision of [Carole Lartizien](https://www.creatis.insa-lyon.fr/~lartizien/) and [Ievgen Redko](https://ievred.github.io/). 

I am reachable at **sofiane.myLastName[At]gmail.com** (my first name has a different spelling on my e-mail).

# Publications
* [Meta Learning in Bandits within shared affine Subspaces](https://proceedings.mlr.press/v238/bilaj24a.html), AISTATS 2024 (2nd author)
* [Hypothesis Transfer in Bandits by Weighted Models](https://link.springer.com/chapter/10.1007/978-3-031-26412-2_18), ECML PKDD 2022 (2nd author)
* [Contributions to unsupervised domain adaptation : Similarity functions, optimal transport and theoretical guarantees](https://theses.hal.science/tel-03199646)
* [Margin-aware Adversarial Domain Adaptation with Optimal Transport](http://proceedings.mlr.press/v119/dhouib20b.html), ICML 2020
* [A Swiss Army Knife for Minimax Optimal Transport](http://proceedings.mlr.press/v119/dhouib20a.html), ICML 2020
* [Analyse théorique de l’apprentissage avec des fonctions de similarités pour l’adaptation de domaine](https://hal.archives-ouvertes.fr/hal-02063285), CAp 2018 (France)
* [On learning a large margin classifier for domain adaptation based on similarity functions](https://hal.archives-ouvertes.fr/hal-02343988), CAp 2019 (France)
* [Revisiting (\epsilon, \gamma, \tau)-similarity learning for domain adaptation](https://papers.nips.cc/paper/7969-revisiting-epsilon-gamma-tau-similarity-learning-for-domain-adaptation), NeurIPS 2018

# Preprints
* [Connecting sufficient conditions for domain adaptation: source-guided uncertainty, relaxed divergences and discrepancy localization](https://arxiv.org/abs/2203.05076), arXiv preprint, 2022.

# Reviewing:
* ICML 2021
* Neurips 2021: Outstanding Reviewer (Top 8% of reviewers).
* ICLR 2022
* Neurips 2023: Outstanding Reviewer (Top 8% of reviewers).
* TMLR

# Some useful links:
A [list of links](useful_links.md) that I found were useful during my PhD years: tools, GitHub repositories, dataset download links ...
